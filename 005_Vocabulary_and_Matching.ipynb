{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "005-Vocabulary-and-Matching.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMwOGPwhp1DAMArJWf4MgKp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noircir/Python/blob/master/005_Vocabulary_and_Matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0x5td16rExd",
        "colab_type": "text"
      },
      "source": [
        "# Vocabulary and Matching\n",
        "So far we've seen how a body of text is divided into tokens, and how individual tokens are parsed and tagged with parts of speech, dependencies and lemmas.\n",
        "\n",
        "In this section we will identify and label specific phrases that match patterns we can define ourselves. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2UhN7p_rK5_",
        "colab_type": "text"
      },
      "source": [
        "## Rule-based Matching\n",
        "spaCy offers a rule-matching tool called `Matcher` that allows you to build a library of token patterns, then match those patterns against a Doc object to return a list of found matches. You can match on any part of the token including text and annotations, and you can add multiple patterns to the same matcher."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKID3ZU4qwcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqJio_lirrC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the Matcher library\n",
        "from spacy.matcher import Matcher\n",
        "#Here matcher is an object that pairs to the current Vocab object. \n",
        "# We can add and remove specific named matchers to matcher as needed.\n",
        "matcher = Matcher(nlp.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7npkaWTsnuj",
        "colab_type": "text"
      },
      "source": [
        "### Creating patterns\n",
        "In literature, the phrase 'solar power' might appear as one word or two, with or without a hyphen. In this section we'll develop a matcher named 'SolarPower' that finds all three:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOQoyreOrw_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SolarPower\n",
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "# Solar power\n",
        "pattern2 = [{'LOWER': 'solar'}, {'LOWER': 'power'}]\n",
        "# Solar-power\n",
        "pattern3 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]\n",
        "\n",
        "matcher.add('SolarPower_matcher', None, pattern1, pattern2, pattern3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhMgfnhBu0w2",
        "colab_type": "text"
      },
      "source": [
        "<table><tr><th>Attribute</th><th>Description</th></tr>\n",
        "\n",
        "<tr ><td><span >`ORTH`</span></td><td>The exact verbatim text of a token</td></tr>\n",
        "<tr ><td><span >`LOWER`</span></td><td>The lowercase form of the token text</td></tr>\n",
        "<tr ><td><span >`LENGTH`</span></td><td>The length of the token text</td></tr>\n",
        "<tr ><td><span >`IS_ALPHA`, `IS_ASCII`, `IS_DIGIT`</span></td><td>Token text consists of alphanumeric characters, ASCII characters, digits</td></tr>\n",
        "<tr ><td><span >`IS_LOWER`, `IS_UPPER`, `IS_TITLE`</span></td><td>Token text is in lowercase, uppercase, titlecase</td></tr>\n",
        "<tr ><td><span >`IS_PUNCT`, `IS_SPACE`, `IS_STOP`</span></td><td>Token is punctuation, whitespace, stop word</td></tr>\n",
        "<tr ><td><span >`LIKE_NUM`, `LIKE_URL`, `LIKE_EMAIL`</span></td><td>Token text resembles a number, URL, email</td></tr>\n",
        "<tr ><td><span >`POS`, `TAG`, `DEP`, `LEMMA`, `SHAPE`</span></td><td>The token's simple and extended part-of-speech tag, dependency label, lemma, shape</td></tr>\n",
        "<tr ><td><span >`ENT_TYPE`</span></td><td>The token's entity label</td></tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_bv6hLAvWhr",
        "colab_type": "text"
      },
      "source": [
        "### Applying the matcher to a Doc object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV8CvuBhsrsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(u'The Solar Power industry continues to grow as demand \\\n",
        "for solarpower increases. Solar-power cars are gaining popularity. Solar power is the future.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMRDfO7Xvanw",
        "colab_type": "code",
        "outputId": "9e892923-9938-451f-f4be-9fb95e8e1f81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "found_matches = matcher(doc)\n",
        "print(found_matches)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(18108190749154820851, 1, 3), (18108190749154820851, 10, 11), (18108190749154820851, 13, 16), (18108190749154820851, 21, 23)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlyP6HdTvdYC",
        "colab_type": "code",
        "outputId": "684d2c3c-32cf-4d20-efa0-1fd542b1a006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "for match_id, start, end in found_matches:\n",
        "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "    span = doc[start:end]                    # get the matched span\n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18108190749154820851 SolarPower_matcher 1 3 Solar Power\n",
            "18108190749154820851 SolarPower_matcher 10 11 solarpower\n",
            "18108190749154820851 SolarPower_matcher 13 16 Solar-power\n",
            "18108190749154820851 SolarPower_matcher 21 23 Solar power\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXDs5uzuwIBE",
        "colab_type": "text"
      },
      "source": [
        "The `match_id` is simply the hash value of the `string_ID` 'SolarPower'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yL9hrLewQjL",
        "colab_type": "text"
      },
      "source": [
        "### Setting pattern options and quantifiers\n",
        "You can make token rules optional by passing an `'OP':'*'` argument. This lets us streamline our patterns list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQuKl27hvzci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Redefine the patterns:\n",
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n",
        "\n",
        "# Remove the old patterns to avoid duplication:\n",
        "matcher.remove('SolarPower_matcher')\n",
        "\n",
        "# Add the new set of patterns to the 'SolarPower' matcher:\n",
        "matcher.add('Another_SolarPower_matcher', None, pattern1, pattern2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvkcDHQtwZS7",
        "colab_type": "code",
        "outputId": "b6d78038-dd3f-45ed-992c-1c35f64028b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "found_matches = matcher(doc)\n",
        "print(found_matches)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(3248665728714946131, 1, 3), (3248665728714946131, 10, 11), (3248665728714946131, 13, 16), (3248665728714946131, 21, 23)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJJRMeRCatG3",
        "colab_type": "code",
        "outputId": "5e1445d8-fe2d-4a69-cf2a-f946ebc1da60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "for match_id, start, end in found_matches:\n",
        "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "    span = doc[start:end]                    # get the matched span\n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3248665728714946131 Another_SolarPower_matcher 1 3 Solar Power\n",
            "3248665728714946131 Another_SolarPower_matcher 10 11 solarpower\n",
            "3248665728714946131 Another_SolarPower_matcher 13 16 Solar-power\n",
            "3248665728714946131 Another_SolarPower_matcher 21 23 Solar power\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08Ll1DNhb9vZ",
        "colab_type": "text"
      },
      "source": [
        "The following quantifiers can be passed to the `'OP'` key:\n",
        "<table><tr><th>OP</th><th>Description</th></tr>\n",
        "\n",
        "<tr ><td><span >\\!</span></td><td>Negate the pattern, by requiring it to match exactly 0 times</td></tr>\n",
        "<tr ><td><span >?</span></td><td>Make the pattern optional, by allowing it to match 0 or 1 times</td></tr>\n",
        "<tr ><td><span >\\+</span></td><td>Require the pattern to match 1 or more times</td></tr>\n",
        "<tr ><td><span >\\*</span></td><td>Allow the pattern to match zero or more times</td></tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpwTNVyFfeFC",
        "colab_type": "text"
      },
      "source": [
        "### Be careful with lemmas!\n",
        "If we wanted to match on both 'solar power' and 'solar powered', it might be tempting to look for the *lemma* of 'powered' and expect it to be 'power'. This is not always the case! The lemma of the *adjective* 'powered' is still 'powered':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_7UKKKaa0GL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LEMMA': 'power'}] # CHANGE THIS PATTERN\n",
        "\n",
        "# Remove the old patterns to avoid duplication:\n",
        "#matcher.remove('SolarPower')\n",
        "\n",
        "# Add the new set of patterns to the 'SolarPower' matcher:\n",
        "matcher.add('SolarPower2', None, pattern2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4VOsOYjfhD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc3 = nlp(u'Solar-powered energy runs solar-powering vehicles.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBxT1MMZfqio",
        "colab_type": "code",
        "outputId": "63a32943-e6b0-4abf-e137-860a13c162ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#matcher.remove('SolarPower')\n",
        "#matcher.add('SolarPower2', None, pattern1, pattern2)\n",
        "found_matches = matcher(doc3)\n",
        "print(found_matches)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(3587797832942597511, 0, 3), (3587797832942597511, 5, 8)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_37ox-Iftbm",
        "colab_type": "code",
        "outputId": "bfc475e8-4f3f-48f3-ac69-f51b01b29df2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for match_id, start, end in found_matches:\n",
        "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "    span = doc[start:end]                    # get the matched span\n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3587797832942597511 SolarPower2 0 3 The Solar Power\n",
            "3587797832942597511 SolarPower2 5 8 to grow as\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAoK0pdNhC3J",
        "colab_type": "text"
      },
      "source": [
        "## Other token attributes\n",
        "Besides lemmas, there are a variety of token attributes we can use to determine matching rules:\n",
        "<table><tr><th>Attribute</th><th>Description</th></tr>\n",
        "\n",
        "<tr ><td><span >`ORTH`</span></td><td>The exact verbatim text of a token</td></tr>\n",
        "<tr ><td><span >`LOWER`</span></td><td>The lowercase form of the token text</td></tr>\n",
        "<tr ><td><span >`LENGTH`</span></td><td>The length of the token text</td></tr>\n",
        "<tr ><td><span >`IS_ALPHA`, `IS_ASCII`, `IS_DIGIT`</span></td><td>Token text consists of alphanumeric characters, ASCII characters, digits</td></tr>\n",
        "<tr ><td><span >`IS_LOWER`, `IS_UPPER`, `IS_TITLE`</span></td><td>Token text is in lowercase, uppercase, titlecase</td></tr>\n",
        "<tr ><td><span >`IS_PUNCT`, `IS_SPACE`, `IS_STOP`</span></td><td>Token is punctuation, whitespace, stop word</td></tr>\n",
        "<tr ><td><span >`LIKE_NUM`, `LIKE_URL`, `LIKE_EMAIL`</span></td><td>Token text resembles a number, URL, email</td></tr>\n",
        "<tr ><td><span >`POS`, `TAG`, `DEP`, `LEMMA`, `SHAPE`</span></td><td>The token's simple and extended part-of-speech tag, dependency label, lemma, shape</td></tr>\n",
        "<tr ><td><span >`ENT_TYPE`</span></td><td>The token's entity label</td></tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIO2XUGchIo_",
        "colab_type": "text"
      },
      "source": [
        "### Token wildcard\n",
        "You can pass an empty dictionary `{}` as a wildcard to represent **any token**. For example, you might want to retrieve hashtags without knowing what might follow the `#` character:\n",
        ">`[{'ORTH': '#'}, {}]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WutPOtgGhUjW",
        "colab_type": "text"
      },
      "source": [
        "___\n",
        "## PhraseMatcher\n",
        "In the above section we used token patterns to perform rule-based matching. An alternative - and often more efficient - method is to match on terminology lists. In this case we use PhraseMatcher to create a Doc object from a list of phrases, and pass that into `matcher` instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KfbZFfBzOqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform standard imports, reset nlp\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udbTKRQKgAVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.matcher import PhraseMatcher\n",
        "matcher = PhraseMatcher(nlp.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9w9SI5ujdJF",
        "colab_type": "text"
      },
      "source": [
        "For this exercise we're going to import a Wikipedia article on Reaganomics\n",
        "Source: https://en.wikipedia.org/wiki/Reaganomics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KwRBED0kebs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install html2text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLaw2fdtkij_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Get the text of the Wiki article\n",
        "\n",
        "import requests\n",
        "#from bs4 import BeautifulSoup\n",
        "link = \"https://en.wikipedia.org/wiki/Reaganomics\"\n",
        "html = requests.get(link).text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPj7plXcldlp",
        "colab_type": "code",
        "outputId": "2267fcc8-076e-4cdd-e982-dd47c1ce0a2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "html[:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<!DOCTYPE html>\\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\"/>\\n<title'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQm-vOErlikJ",
        "colab_type": "code",
        "outputId": "233fdd07-928f-496d-cc96-42d864958434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "import html2text\n",
        "text = html2text.html2text(html)\n",
        "print(text[11000:12000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nixon\")'s [wage and price\n",
            "controls](/wiki/Wage_and_price_controls#United_States \"Wage and price\n",
            "controls\") were phased out.[7] The [federal oil\n",
            "reserves](/wiki/Strategic_Petroleum_Reserve_\\(United_States\\) \"Strategic\n",
            "Petroleum Reserve \\(United States\\)\") were created to ease any future short\n",
            "term shocks. President [Jimmy Carter](/wiki/Jimmy_Carter \"Jimmy Carter\") had\n",
            "begun phasing out price controls on petroleum while he created the Department\n",
            "of Energy. Much of the credit for the resolution of the stagflation is given\n",
            "to two causes: a three-year contraction of the money supply by the [Federal\n",
            "Reserve Board](/wiki/Federal_Reserve_Board \"Federal Reserve Board\") under\n",
            "[Paul Volcker](/wiki/Paul_Volcker \"Paul Volcker\"), initiated in the last year\n",
            "of Carter's presidency, and long-term easing of supply and pricing in oil\n",
            "during the [1980s oil glut](/wiki/1980s_oil_glut \"1980s oil glut\").[\n",
            "_[citation needed](/wiki/Wikipedia:Citation_needed \"Wikipedia:Citation\n",
            "needed\")_ ]\n",
            "\n",
            "In stating that his \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIqs6FaF0_H6",
        "colab_type": "code",
        "outputId": "d7a9a037-e9ec-4c0c-cda1-ff9ee09ad15b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# An example from Spacy documentation https://spacy.io/usage/rule-based-matching#phrasematcher-attrs\n",
        "\n",
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "terms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]\n",
        "# Only run nlp.make_doc to speed things up\n",
        "patterns = [nlp.make_doc(text) for text in terms]\n",
        "matcher.add(\"TerminologyList\", None, *patterns)\n",
        "\n",
        "doc = nlp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
        "          \"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
        "matches = matcher(doc)\n",
        "matches\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    print(span.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Angela Merkel\n",
            "Barack Obama\n",
            "Washington, D.C.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShbtUC921Dv0",
        "colab_type": "code",
        "outputId": "fea049bb-f44a-463b-aefa-77d60bf73998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Need to reset nlp and document for another attempt.\n",
        "# Addition of an attribute ( attr=\"LOWER\" ) makes our search case-insensitive\n",
        "\n",
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "terms = [\"supply-side economics\", \"trickle-down economics\"]\n",
        "\n",
        "# Only run nlp.make_doc to speed things up\n",
        "patterns = [nlp.make_doc(text) for text in terms]\n",
        "matcher.add(\"oooo\", None, *patterns)\n",
        "\n",
        "doc = nlp(text)\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    print(span.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Supply-side economics\n",
            "Supply-side economics\n",
            "Trickle-Down Economics\n",
            "Supply-side economics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBNwblrD1aT_",
        "colab_type": "code",
        "outputId": "b851da57-501f-4320-bd75-7e38aeea5651",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "for match_id, start, end in matches:\n",
        "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "    span = doc_reaganomics[start:end]                    # get the matched span\n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "231001732085949855 oooo 1218 1222 Supply-side economics\n",
            "231001732085949855 oooo 2647 2651 Supply-side economics\n",
            "231001732085949855 oooo 13645 13649 Trickle-Down Economics\n",
            "231001732085949855 oooo 20710 20714 Supply-side economics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwGTCAS-4btA",
        "colab_type": "text"
      },
      "source": [
        "## Viewing Matches\n",
        "There are a few ways to fetch the text surrounding a match. The simplest is to grab a slice of tokens from the doc that is wider than the match:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czkcZrj04XR8",
        "colab_type": "code",
        "outputId": "1b324fa0-35bc-4fdf-d068-f2bd7f14726a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "doc_reaganomics[2620:2670]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Deal\") policies. At the same time he attracted a\n",
              "following from the [supply-side economics](/wiki/Supply-side_economics\n",
              "\"Supply-side economics\") movement, which formed in opposition to\n",
              "[Keynesian](/wiki/Keynesian \"Keynesian\") demand"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dROMlkiA5Gq3",
        "colab_type": "text"
      },
      "source": [
        "Another way is to first apply the `sentencizer` to the Doc, then iterate through the sentences to the match point:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf14bbD66N2T",
        "colab_type": "code",
        "outputId": "a2142cf6-f6ec-4d86-998a-6ebde629453b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sents = [sent for sent in doc_reaganomics.sents]\n",
        "print(sents[0].start, sents[0].end)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwDIQkU64wNb",
        "colab_type": "code",
        "outputId": "ad474eb2-7439-4c60-aefd-9e98033db293",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for sent in sents:\n",
        "    if matches[3][1] < sent.end:  # this is the fourth match, that starts at doc_reagomics[20710]\n",
        "        print(sent)\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Supply-side](/wiki/Supply-side_economics \"Supply-side economics\")\n",
            "  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dabVarAi5nS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}