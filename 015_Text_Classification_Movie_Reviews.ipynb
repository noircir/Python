{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "015-Text-Classification-Movie-Reviews.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noircir/Python/blob/master/015_Text_Classification_Movie_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXTlNEAIuuJq",
        "colab_type": "text"
      },
      "source": [
        "# Text Classification: predict if a movie review is \"positive\" or \"negative\"\n",
        "\n",
        "* Read in a collection of documents - a *corpus*\n",
        "* Transform text into numerical vector data using a pipeline\n",
        "* Create a classifier\n",
        "* Fit/train the classifier\n",
        "* Test the classifier on new data\n",
        "* Evaluate performance\n",
        "\n",
        "Cornell University Movie Review polarity dataset v2.0 obtained from http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
        "\n",
        "We'll try to predict the Positive/Negative labels based on text content alone. Later, we'll apply *Sentiment Analysis* to train models that have a deeper understanding of each review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdMM9W8luuJr",
        "colab_type": "text"
      },
      "source": [
        "## Perform imports and load the dataset\n",
        "The dataset contains the text of 2000 movie reviews. 1000 are positive, 1000 are negative, and the text has been preprocessed as a tab-delimited file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwyfg5AaxBbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WjoP-7quuJt",
        "colab_type": "code",
        "outputId": "26176b27-2af7-4c5b-da48-d86bc6b8f243",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/NLP-Spacy/TextFiles/moviereviews.tsv', sep='\\t')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neg</td>\n",
              "      <td>how do films like mouse hunt get into theatres...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neg</td>\n",
              "      <td>some talented actresses are blessed with a dem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pos</td>\n",
              "      <td>this has been an extraordinary year for austra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pos</td>\n",
              "      <td>according to hollywood movies made in last few...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>neg</td>\n",
              "      <td>my first press screening of 1998 and already i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                             review\n",
              "0   neg  how do films like mouse hunt get into theatres...\n",
              "1   neg  some talented actresses are blessed with a dem...\n",
              "2   pos  this has been an extraordinary year for austra...\n",
              "3   pos  according to hollywood movies made in last few...\n",
              "4   neg  my first press screening of 1998 and already i..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9o9LERduuJz",
        "colab_type": "code",
        "outputId": "2428e8af-1d74-4e62-936d-9affebe56c4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzX9r--_uuJ2",
        "colab_type": "text"
      },
      "source": [
        "### Take a look at a typical review. This one is labeled \"negative\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_zYWXWzuuJ4",
        "colab_type": "code",
        "outputId": "d96eb916-2f08-46c3-e2bf-30d044888d33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "print(df['review'][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "how do films like mouse hunt get into theatres ? \r\n",
            "isn't there a law or something ? \r\n",
            "this diabolical load of claptrap from steven speilberg's dreamworks studio is hollywood family fare at its deadly worst . \r\n",
            "mouse hunt takes the bare threads of a plot and tries to prop it up with overacting and flat-out stupid slapstick that makes comedies like jingle all the way look decent by comparison . \r\n",
            "writer adam rifkin and director gore verbinski are the names chiefly responsible for this swill . \r\n",
            "the plot , for what its worth , concerns two brothers ( nathan lane and an appalling lee evens ) who inherit a poorly run string factory and a seemingly worthless house from their eccentric father . \r\n",
            "deciding to check out the long-abandoned house , they soon learn that it's worth a fortune and set about selling it in auction to the highest bidder . \r\n",
            "but battling them at every turn is a very smart mouse , happy with his run-down little abode and wanting it to stay that way . \r\n",
            "the story alternates between unfunny scenes of the brothers bickering over what to do with their inheritance and endless action sequences as the two take on their increasingly determined furry foe . \r\n",
            "whatever promise the film starts with soon deteriorates into boring dialogue , terrible overacting , and increasingly uninspired slapstick that becomes all sound and fury , signifying nothing . \r\n",
            "the script becomes so unspeakably bad that the best line poor lee evens can utter after another run in with the rodent is : \" i hate that mouse \" . \r\n",
            "oh cringe ! \r\n",
            "this is home alone all over again , and ten times worse . \r\n",
            "one touching scene early on is worth mentioning . \r\n",
            "we follow the mouse through a maze of walls and pipes until he arrives at his makeshift abode somewhere in a wall . \r\n",
            "he jumps into a tiny bed , pulls up a makeshift sheet and snuggles up to sleep , seemingly happy and just wanting to be left alone . \r\n",
            "it's a magical little moment in an otherwise soulless film . \r\n",
            "a message to speilberg : if you want dreamworks to be associated with some kind of artistic credibility , then either give all concerned in mouse hunt a swift kick up the arse or hire yourself some decent writers and directors . \r\n",
            "this kind of rubbish will just not do at all . \r\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0oNk5WBuuJ7",
        "colab_type": "text"
      },
      "source": [
        "## Check for missing values:\n",
        "There are records with missing data. Some have NaN values, others have short strings composed of only spaces. This might happen if a reviewer declined to provide a comment with their review. We will show two ways using pandas to identify and remove records containing empty data.\n",
        "* NaN records are efficiently handled with [.isnull()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.isnull.html) and [.dropna()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html)\n",
        "* Strings that contain only whitespace can be handled with [.isspace()](https://docs.python.org/3/library/stdtypes.html#str.isspace), [.itertuples()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.itertuples.html), and [.drop()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html)\n",
        "\n",
        "### Detect & remove NaN values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRYsy2d3uuJ8",
        "colab_type": "code",
        "outputId": "b5e70c73-34a2-4447-b075-a4289a84f1b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Check for the existence of NaN values in a cell:\n",
        "df.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label      0\n",
              "review    35\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd3Aq6ucuuKB",
        "colab_type": "text"
      },
      "source": [
        "35 records show **NaN** (this stands for \"not a number\" and is equivalent to *None*). These are easily removed using the `.dropna()` pandas function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXo9fDUauuKD",
        "colab_type": "code",
        "outputId": "8fcaeb05-c56b-4332-e512-cd6e97a4ee14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.dropna(inplace=True)\n",
        "\n",
        "len(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1965"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOJVul2-uuKH",
        "colab_type": "text"
      },
      "source": [
        "### Detect & remove empty strings\n",
        "Technically, we're dealing with \"whitespace only\" strings. If the original .tsv file had contained empty strings, pandas **.read_csv()** would have assigned NaN values to those cells by default.\n",
        "\n",
        "In order to detect these strings we need to iterate over each row in the DataFrame. The **.itertuples()** pandas method is a good tool for this as it provides access to every field. For brevity we'll assign the names `i`, `lb` and `rv` to the `index`, `label` and `review` columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsVhgOgbuuKI",
        "colab_type": "code",
        "outputId": "ce5fdae7-b548-443f-f465-a9a5aeb2d7f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "blanks = []  # start with an empty list\n",
        "\n",
        "for index,label,review in df.itertuples():  # iterate over the DataFrame\n",
        "    if type(review)==str:            # avoid NaN values\n",
        "        if review.isspace():         # test 'review' for whitespace\n",
        "            blanks.append(index)     # add matching index numbers to the list\n",
        "        \n",
        "print(len(blanks), 'blanks: ', blanks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27 blanks:  [57, 71, 147, 151, 283, 307, 313, 323, 343, 351, 427, 501, 633, 675, 815, 851, 977, 1079, 1299, 1455, 1493, 1525, 1531, 1763, 1851, 1905, 1993]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYReI9A_uuKL",
        "colab_type": "text"
      },
      "source": [
        "Next we'll pass our list of index numbers to the **.drop()** method, and set `inplace=True` to make the change permanent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZfeH2YTuuKM",
        "colab_type": "code",
        "outputId": "079d1979-ab5d-4f16-9787-dba33921bda8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.drop(blanks, inplace=True)\n",
        "\n",
        "len(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1938"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__EQDhFFuuKQ",
        "colab_type": "text"
      },
      "source": [
        "Great! We dropped 62 records from the original 2000. Let's continue with the analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfVL214XuuKR",
        "colab_type": "text"
      },
      "source": [
        "## Take a quick look at the `label` column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gicLT1fhuuKT",
        "colab_type": "code",
        "outputId": "be1c02b5-8451-483f-cbe7-219bfab36414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "df['label'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neg    969\n",
              "pos    969\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN9wTXhvuuKW",
        "colab_type": "text"
      },
      "source": [
        "## Split the data into train & test sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6GkFAwkuuKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['review']\n",
        "y = df['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJGBGWbvuuKa",
        "colab_type": "text"
      },
      "source": [
        "## Build pipelines to vectorize the data, then train and fit a model\n",
        "Now that we have sets to train and test, we'll develop a selection of pipelines, each with a different model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CBLP8rRuuKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Naïve Bayes:\n",
        "text_clf_nb = Pipeline([('tfidf', TfidfVectorizer()),\n",
        "                     ('clf', MultinomialNB()),\n",
        "])\n",
        "\n",
        "# Linear SVC:\n",
        "text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer()),\n",
        "                     ('clf', LinearSVC()),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tQ3jwbOuuKe",
        "colab_type": "text"
      },
      "source": [
        "## Feed the training data through the first pipeline\n",
        "We'll run naïve Bayes first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_HU8Da-uuKf",
        "colab_type": "code",
        "outputId": "225d4201-13d9-4f26-c616-ba10f19dfca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "text_clf_nb.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('clf',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nm7NFO_uuKj",
        "colab_type": "text"
      },
      "source": [
        "## Run predictions and analyze the results (naïve Bayes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKWsncTZuuKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Form a prediction set\n",
        "predictions = text_clf_nb.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-_MKIz0uuKo",
        "colab_type": "code",
        "outputId": "0e19b473-19fd-4020-9692-ee5d40a96f27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Report the confusion matrix\n",
        "from sklearn import metrics\n",
        "print(metrics.confusion_matrix(y_test,predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[287  21]\n",
            " [130 202]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPyk4dhBuuKq",
        "colab_type": "code",
        "outputId": "e2cc29cc-4b51-45d5-b67d-ceeb10c2527b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Print a classification report\n",
        "print(metrics.classification_report(y_test,predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.69      0.93      0.79       308\n",
            "         pos       0.91      0.61      0.73       332\n",
            "\n",
            "    accuracy                           0.76       640\n",
            "   macro avg       0.80      0.77      0.76       640\n",
            "weighted avg       0.80      0.76      0.76       640\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPRzgLI1uuKt",
        "colab_type": "code",
        "outputId": "bf187728-279f-4168-e4e7-f31646099497",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Print the overall accuracy\n",
        "print(metrics.accuracy_score(y_test,predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7640625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76YGSWgSuuKw",
        "colab_type": "text"
      },
      "source": [
        "Naïve Bayes gave us better-than-average results at 76.4% for classifying reviews as positive or negative based on text alone. Let's see if we can do better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_cm0Ea8uuKx",
        "colab_type": "text"
      },
      "source": [
        "## Feed the training data through the second pipeline\n",
        "Next we'll run Linear SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z7OrgqpuuKy",
        "colab_type": "code",
        "outputId": "6fa63615-2133-4dbc-9783-038ddf16ea1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "text_clf_lsvc.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('clf',\n",
              "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
              "                           fit_intercept=True, intercept_scaling=1,\n",
              "                           loss='squared_hinge', max_iter=1000,\n",
              "                           multi_class='ovr', penalty='l2', random_state=None,\n",
              "                           tol=0.0001, verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V_8XuFjuuK2",
        "colab_type": "text"
      },
      "source": [
        "## Run predictions and analyze the results (Linear SVC)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZO1MFetuuK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Form a prediction set\n",
        "predictions = text_clf_lsvc.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgUMdA_OuuK6",
        "colab_type": "code",
        "outputId": "21b0719c-12f6-4c20-c176-40341e831181",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Report the confusion matrix\n",
        "from sklearn import metrics\n",
        "print(metrics.confusion_matrix(y_test,predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[259  49]\n",
            " [ 49 283]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDV1ahJUuuK9",
        "colab_type": "code",
        "outputId": "062f0070-d489-46b4-b5e4-2689f132fcda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Print a classification report\n",
        "print(metrics.classification_report(y_test,predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.84      0.84      0.84       308\n",
            "         pos       0.85      0.85      0.85       332\n",
            "\n",
            "    accuracy                           0.85       640\n",
            "   macro avg       0.85      0.85      0.85       640\n",
            "weighted avg       0.85      0.85      0.85       640\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmU8Zs-quuLA",
        "colab_type": "code",
        "outputId": "40fbc7dd-b3d5-41fa-a374-cca673b37c01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Print the overall accuracy\n",
        "print(metrics.accuracy_score(y_test,predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.846875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMkm6tKguuLD",
        "colab_type": "text"
      },
      "source": [
        "Not bad! Based on text alone we correctly classified reviews as positive or negative **84.7%** of the time. Later we'll try to improve this score even further by performing *sentiment analysis* on the reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDboHOqDuuLF",
        "colab_type": "text"
      },
      "source": [
        "## Advanced Topic - Adding Stopwords to CountVectorizer\n",
        "By default, **CountVectorizer** and **TfidfVectorizer** do *not* filter stopwords. However, they offer some optional settings, including passing in your own stopword list.\n",
        "<div class=\"alert alert-info\" style=\"margin: 20px\">CAUTION: There are some [known issues](http://aclweb.org/anthology/W18-2502) using Scikit-learn's built-in stopwords list. Some words that are filtered may in fact aid in classification. In this section we'll pass in our own stopword list, so that we know exactly what's being filtered.</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij1U2dvcuuLF",
        "colab_type": "text"
      },
      "source": [
        "The [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class accepts the following arguments:\n",
        "> *CountVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, **stop_words=None**, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)*\n",
        "\n",
        "[TfidVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) supports the same arguments and more. Under *stop_words* we have the following options:\n",
        "> stop_words : *string {'english'}, list, or None (default)*\n",
        "\n",
        "That is, we can run `TfidVectorizer(stop_words='english')` to accept scikit-learn's built-in list,<br>\n",
        "or `TfidVectorizer(stop_words=[a, and, the])` to filter these three words. In practice we would assign our list to a variable and pass that in instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILWC9Qb8uuLG",
        "colab_type": "text"
      },
      "source": [
        "Scikit-learn's built-in list contains 318 stopwords:\n",
        "> <pre>from sklearn.feature_extraction import text\n",
        "> print(text.ENGLISH_STOP_WORDS)</pre>\n",
        "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX3-7NwX6DxV",
        "colab_type": "code",
        "outputId": "46271139-0348-4d7b-f08d-de353c58f318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from sklearn.feature_extraction import text\n",
        "print(text.ENGLISH_STOP_WORDS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frozenset({'former', 'while', 'not', 'becoming', 'hundred', 'something', 'was', 'neither', 'else', 'cannot', 'she', 'none', 'whatever', 'for', 'fill', 'ever', 'other', 'last', 'him', 'six', 'thick', 'thereupon', 'been', 'twelve', 'thence', 'somehow', 'always', 'whereby', 'via', 'about', 'below', 'beside', 'therefore', 'have', 'serious', 'thus', 'though', 'up', 'keep', 'show', 'someone', 'anywhere', 'namely', 'through', 'whither', 'thru', 'me', 'otherwise', 'seemed', 'more', 'whence', 'full', 'still', 'ours', 'see', 'yours', 'please', 'these', 'ie', 'fire', 'some', 'where', 'herself', 'behind', 'due', 'myself', 'nine', 're', 'along', 'interest', 'against', 'to', 'cant', 'be', 'are', 'beforehand', 'bottom', 'do', 'both', 'themselves', 'find', 'amount', 'why', 'who', 'empty', 'whose', 'several', 'take', 'done', 'hereafter', 'is', 'her', 'made', 'am', 'may', 'moreover', 'nor', 'by', 'above', 'name', 'ourselves', 'however', 'will', 'our', 'toward', 'mine', 'go', 'anyhow', 'became', 'elsewhere', 'nothing', 'down', 'out', 'already', 'each', 'hasnt', 'all', 'everything', 'amongst', 'least', 'rather', 'without', 'itself', 'wherever', 'inc', 'seeming', 'seem', 'it', 'also', 'somewhere', 'hence', 'here', 'indeed', 'anything', 'nevertheless', 'among', 'no', 'those', 'would', 'whenever', 'had', 'across', 'mostly', 'together', 'whether', 'with', 'two', 'found', 'because', 'any', 'per', 'you', 'can', 'has', 'on', 'either', 'even', 'fifty', 'eight', 'its', 'whereas', 'give', 'whole', 'such', 'enough', 'ltd', 'one', 'besides', 'might', 'yet', 'within', 'off', 'and', 'after', 'sixty', 'my', 'nowhere', 'of', 'i', 'at', 'beyond', 'often', 'ten', 'top', 'towards', 'eleven', 'so', 'put', 'whereafter', 'whereupon', 'before', 'although', 'only', 'hereupon', 'hers', 'again', 'that', 'could', 'seems', 'twenty', 'own', 'an', 'what', 'between', 'formerly', 'same', 'whoever', 'herein', 'become', 'four', 'nobody', 'once', 'never', 'from', 'much', 'alone', 'must', 'under', 'since', 'every', 'co', 'now', 'few', 'over', 'becomes', 'everyone', 'if', 'latter', 'con', 'than', 'this', 'perhaps', 'sometimes', 'yourselves', 'three', 'around', 'forty', 'sincere', 'the', 'thin', 'un', 'everywhere', 'in', 'eg', 'should', 'bill', 'during', 'they', 'how', 'latterly', 'afterwards', 'anyone', 'into', 'others', 'etc', 'detail', 'first', 'too', 'thereafter', 'upon', 'as', 'third', 'further', 'describe', 'well', 'when', 'whom', 'almost', 'move', 'wherein', 'de', 'but', 'cry', 'hereby', 'get', 'anyway', 'onto', 'there', 'except', 'fifteen', 'himself', 'his', 'a', 'amoungst', 'side', 'five', 'us', 'very', 'were', 'yourself', 'being', 'next', 'therein', 'most', 'front', 'then', 'couldnt', 'he', 'part', 'noone', 'call', 'meanwhile', 'system', 'back', 'or', 'throughout', 'until', 'them', 'less', 'sometime', 'we', 'thereby', 'your', 'which', 'their', 'another', 'mill', 'many'})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Er3q7La6HMi",
        "colab_type": "code",
        "outputId": "d7c052cc-6781-431d-b6fc-d0e54ecdfea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(text.ENGLISH_STOP_WORDS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "318"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5FHW0ZD6UZA",
        "colab_type": "text"
      },
      "source": [
        "However, there are words in this list that may influence a classification of movie reviews. With this in mind, let's trim the list to just 60 words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3FI5x-xuuLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords = ['a', 'about', 'an', 'and', 'are', 'as', 'at', 'be', 'been', 'but', 'by', 'can', \\\n",
        "             'even', 'ever', 'for', 'from', 'get', 'had', 'has', 'have', 'he', 'her', 'hers', 'his', \\\n",
        "             'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'just', 'me', 'my', 'of', 'on', 'or', \\\n",
        "             'see', 'seen', 'she', 'so', 'than', 'that', 'the', 'their', 'there', 'they', 'this', \\\n",
        "             'to', 'was', 'we', 'were', 'what', 'when', 'which', 'who', 'will', 'with', 'you']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZwqGnOsuuLK",
        "colab_type": "text"
      },
      "source": [
        "Now let's repeat the process above and see if the removal of stopwords improves or impairs our score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSd4T1ypuuLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YOU DO NOT NEED TO RUN THIS CELL UNLESS YOU HAVE\n",
        "# RECENTLY OPENED THIS NOTEBOOK OR RESTARTED THE KERNEL:\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('../TextFiles/moviereviews.tsv', sep='\\t')\n",
        "df.dropna(inplace=True)\n",
        "blanks = []\n",
        "for i,lb,rv in df.itertuples():\n",
        "    if type(rv)==str:\n",
        "        if rv.isspace():\n",
        "            blanks.append(i)\n",
        "df.drop(blanks, inplace=True)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = df['review']\n",
        "y = df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wInXDYSnuuLR",
        "colab_type": "code",
        "outputId": "1c6890d6-804a-4d5e-f88c-4b0b67ec76da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "# RUN THIS CELL TO ADD STOPWORDS TO THE LINEAR SVC PIPELINE:\n",
        "text_clf_lsvc2 = Pipeline([('tfidf', TfidfVectorizer(stop_words=stopwords)),\n",
        "                     ('clf', LinearSVC()),\n",
        "])\n",
        "text_clf_lsvc2.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=['a', 'about', 'an', 'and', 'are',\n",
              "                                             'as', 'at', 'be', 'been', 'but',...\n",
              "                                             'how', 'i', 'if', 'in', 'into',\n",
              "                                             'is', ...],\n",
              "                                 strip_accents=None, sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('clf',\n",
              "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
              "                           fit_intercept=True, intercept_scaling=1,\n",
              "                           loss='squared_hinge', max_iter=1000,\n",
              "                           multi_class='ovr', penalty='l2', random_state=None,\n",
              "                           tol=0.0001, verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTOTuXQquuLU",
        "colab_type": "code",
        "outputId": "1c0fd8b2-b2b5-446b-ef1d-6b1fe22e6fe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "predictions = text_clf_lsvc2.predict(X_test)\n",
        "print(metrics.confusion_matrix(y_test,predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[256  52]\n",
            " [ 48 284]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUG38p64uuLX",
        "colab_type": "code",
        "outputId": "3e119508-3214-4ae6-eb15-4b8395dcb035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(metrics.classification_report(y_test,predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.84      0.83      0.84       308\n",
            "         pos       0.85      0.86      0.85       332\n",
            "\n",
            "    accuracy                           0.84       640\n",
            "   macro avg       0.84      0.84      0.84       640\n",
            "weighted avg       0.84      0.84      0.84       640\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TspG0aRuuLa",
        "colab_type": "code",
        "outputId": "cfe4d2bf-0ddf-4b6a-8064-ccee57082866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(metrics.accuracy_score(y_test,predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.84375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXNLXbxGuuLe",
        "colab_type": "text"
      },
      "source": [
        "Our score didn't change that much. We went from 84.7% without filtering stopwords to 84.4% after adding a stopword filter to our pipeline. Keep in mind that 2000 movie reviews is a relatively small dataset. The real gain from stripping stopwords is improved processing speed; depending on the size of the corpus, it might save hours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4KRm8X2uuLf",
        "colab_type": "text"
      },
      "source": [
        "## Feed new data into a trained model\n",
        "Once we've developed a fairly accurate model, it's time to feed new data through it. In this last section we'll write our own review, and see how accurately our model assigns a \"positive\" or \"negative\" label to it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYLIY7vHuuLg",
        "colab_type": "text"
      },
      "source": [
        "### First, train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmVvtEjnuuLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YOU DO NOT NEED TO RUN THIS CELL UNLESS YOU HAVE\n",
        "# RECENTLY OPENED THIS NOTEBOOK OR RESTARTED THE KERNEL:\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('../TextFiles/moviereviews.tsv', sep='\\t')\n",
        "df.dropna(inplace=True)\n",
        "blanks = []\n",
        "for i,lb,rv in df.itertuples():\n",
        "    if type(rv)==str:\n",
        "        if rv.isspace():\n",
        "            blanks.append(i)\n",
        "df.drop(blanks, inplace=True)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = df['review']\n",
        "y = df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn import metrics\n",
        "\n",
        "# Naïve Bayes Model:\n",
        "text_clf_nb = Pipeline([('tfidf', TfidfVectorizer()),\n",
        "                     ('clf', MultinomialNB()),\n",
        "])\n",
        "\n",
        "# Linear SVC Model:\n",
        "text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer()),\n",
        "                     ('clf', LinearSVC()),\n",
        "])\n",
        "\n",
        "# Train both models on the moviereviews.tsv training set:\n",
        "text_clf_nb.fit(X_train, y_train)\n",
        "text_clf_lsvc.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU91N8zquuLj",
        "colab_type": "text"
      },
      "source": [
        "### Next, feed new data to the model's `predict()` method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsmZ40_NuuLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "myreview = \"A movie I really wanted to love was terrible. \\\n",
        "I'm sure the producers had the best intentions, but the execution was lacking.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcDCjgI-uuLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use this space to write your own review. Experiment with different lengths and writing styles.\n",
        "myreview = '''\n",
        "Parasite is a difficult film to talk about. It defies any easy pigeonhole, \n",
        "wriggles free from slotting into a single genre, can be considered both a mainstream crowd-pleaser \n",
        "and an arthouse masterpiece — and is, undeniably, a film best enjoyed going in blind, \n",
        "its delicious and shocking surprises ideally experienced as innocently and obliviously as possible. \n",
        "So, finding words to describe it are hard. If there’s one word that can best sum it up, \n",
        "it’s the director: Bong Joon Ho.\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX6Ky4F17hvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "another_review = '''\n",
        "In his latest work, Bong blends a con-man story with a tale of suspense, to uproarious and enlightening effect. \n",
        "The film follows the working-class Kim family, whose cramped basement apartment catches more clouds of fumigation \n",
        "than rays of sunshine. Ki-woo (Choi Woo-shik), the son, is referred by a friend to tutor the daughter of the wealthy Park family.\n",
        "\n",
        "When Ki-woo arrives at their stunning estate — designed, we are told, by a famous architect, and created, in part, \n",
        "on a soundstage and via digital wizardry — his eyes light up with envy at everything he and his family don’t have. \n",
        "It’s the perfect construction for Bong to tear apart, brick by brick.\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3DvjnNLuuLu",
        "colab_type": "code",
        "outputId": "144c502f-7097-4bdd-f023-f525f74e58a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(text_clf_nb.predict([myreview]))  # be sure to put \"myreview\" inside square brackets"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['neg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RzCqEVeuuLx",
        "colab_type": "code",
        "outputId": "9d28b57d-61b6-4d6a-a8d5-a47a571f17e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(text_clf_lsvc.predict([myreview]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['pos']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NLHfo8I7tU1",
        "colab_type": "code",
        "outputId": "f0147d7f-0186-414b-a274-03dc8a6a0613",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(text_clf_nb.predict([another_review]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['pos']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tuKzO_s7tMS",
        "colab_type": "code",
        "outputId": "f514bcc7-23a1-47d3-b2cc-f03788315146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(text_clf_lsvc.predict([another_review]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['pos']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVpuPILruuLz",
        "colab_type": "text"
      },
      "source": [
        "Now we can build text classification pipelines in scikit-learn, apply a variety of algorithms like naïve Bayes and Linear SVC, handle stopwords, and test a fitted model on new data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyzzpNgc64Nu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}