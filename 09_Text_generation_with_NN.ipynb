{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09-Text-generation-with-NN.ipynb",
      "provenance": [],
      "mount_file_id": "1ui3UKc8Iq80wkFXc61obC0ZRu-9wyb7Y",
      "authorship_tag": "ABX9TyNWyBc+r7WpS0bsByEvMLzo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noircir/Python/blob/master/09_Text_generation_with_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeUNhYLLzHsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CAREFUL ! THE model.fit() RUNS FOR ABOUT 2-3 HOURS ON CPU ! CHANGE TO GPU ! (3+ times faster)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfyG4TnO8FwZ",
        "colab_type": "text"
      },
      "source": [
        "# Load text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPPreh2yNsRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install docx2txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-3APuhzNveW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import docx2txt\n",
        "import re\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miV5UACfOPyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compress(text):\n",
        "  '''\n",
        "  removes blank lines and replaces multiple spaces with one space\n",
        "  '''\n",
        "  text = text.replace('\\t', ' ')\n",
        "  return re.sub('\\n+', '\\n', text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GISg0s3DORdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = docx2txt.process ('/content/drive/My Drive/Colab Notebooks/Self-learning chatbot/texts/document16.docx')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqNQWwEmOoYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPeXlzjkS5Eo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = text.replace(u'\\xa0', u' ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Edt6_8ZTBk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9J83Kit9bvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = compress(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAN2rbBm9khD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWyR2lKMO0lX",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize and Clean Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrzLwn1rNkmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SnHbiqGO6Z_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!python -m spacy download fr_core_news_sm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXuXs1DMPJvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To load French vocab, RESTART THE RUNTIME !!\n",
        "\n",
        "nlp = spacy.load('fr_core_news_sm',disable=['parser', 'tagger','ner'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_dpHnyaPTNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (Needs further fine-tuning for multiple blank lines)\n",
        "\n",
        "def separate_punc(doc_text):\n",
        "    return [token.text.lower() for token in nlp(doc_text) \n",
        "    if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n \\n\\n\\t\\t \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\t']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lnIpH8XQcEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = separate_punc(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bJ5b-25QrSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb_atpi_Qsba",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "897dbe7d-eb48-4284-bbea-d1e9df406d60"
      },
      "source": [
        "len(tokens)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14485"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLNvTTO7TZFb",
        "colab_type": "text"
      },
      "source": [
        "## Create Sequences of Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXLYToH6TVy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# organize into sequences of tokens. \n",
        "# A sequence of 20 words (for example), then predict the 21th word. \n",
        "\n",
        "train_len = 20+1 # training words , then one target word\n",
        "\n",
        "# Empty list of sequences\n",
        "text_sequences = []\n",
        "\n",
        "for i in range(train_len, len(tokens)):\n",
        "    \n",
        "    # Grab train_len# amount of characters\n",
        "    seq = tokens[i-train_len:i]\n",
        "    \n",
        "    # Add to list of sequences\n",
        "    text_sequences.append(seq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR8AW5NqTdbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Given 20 words, can you predict the 21st (the last one) ?\n",
        "\n",
        "' '.join(text_sequences[100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u90mkVrLTo4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "' '.join(text_sequences[220])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTFvGoAZTsTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "' '.join(text_sequences[400])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_fy192gTvVW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca1c1569-f5e2-4f12-f1e7-7f81ac0b4e37"
      },
      "source": [
        "len(text_sequences)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14464"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-tASH05T3TK",
        "colab_type": "text"
      },
      "source": [
        "## Keras Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN87zJFoTyEx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "765923d1-69fc-4a16-da8c-32c34f1d5011"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLU0gzZpT6CR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Integer-encode sequences of words\n",
        "# Tokenizer() has many options, including punctiuation and the number of words to be kept...\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_sequences)\n",
        "sequences = tokenizer.texts_to_sequences(text_sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5IdddpoT9zu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "d5ff042d-e59b-4306-c653-b45df700db77"
      },
      "source": [
        "# Each of these numbers is an id for a particular word\n",
        "\n",
        "sequences[0]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[48,\n",
              " 9,\n",
              " 45,\n",
              " 220,\n",
              " 175,\n",
              " 9,\n",
              " 48,\n",
              " 9,\n",
              " 45,\n",
              " 54,\n",
              " 11,\n",
              " 2003,\n",
              " 592,\n",
              " 591,\n",
              " 1,\n",
              " 469,\n",
              " 2001,\n",
              " 11,\n",
              " 142,\n",
              " 468,\n",
              " 34]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1I74xfoUAjB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "975158b7-cf44-4ca0-b8ad-c2f1f6147529"
      },
      "source": [
        "tokenizer.index_word[50]"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'prix'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG-5od3wUC5V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "5b583bad-0a0f-4da9-8f26-9bea40b48a82"
      },
      "source": [
        "for i in sequences[50]:\n",
        "    print(f'{i} : {tokenizer.index_word[i]}')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "152 : documentation\n",
            "9 : d’\n",
            "1132 : appels\n",
            "9 : d’\n",
            "45 : offres\n",
            "142 : biens\n",
            "468 : informatiques\n",
            "34 : logiciel\n",
            "11 : contrat\n",
            "278 : version\n",
            "763 : détaillée\n",
            "764 : 2019\n",
            "173 : 12\n",
            "174 : 20\n",
            "1133 : table\n",
            "17 : des\n",
            "1134 : matières\n",
            "592 : page\n",
            "765 : préambule\n",
            "1135 : 9\n",
            "14 : \n",
            "  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDbdDPdOVHQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Word counts\n",
        "\n",
        "#tokenizer.word_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6KVJnKQVL04",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "701a6591-132a-434c-d8a4-ae92ead55bb8"
      },
      "source": [
        "# Vocabulary size\n",
        "\n",
        "vocabulary_size = len(tokenizer.word_counts)\n",
        "vocabulary_size"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2004"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUJeGoLLVTDi",
        "colab_type": "text"
      },
      "source": [
        "## Convert to Numpy Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GapWRIHFVPtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FTNyvbuVVry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = np.array(sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EaNMEplVYyl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "b11e3f0a-b27b-479c-a8d9-6bcad36f5b57"
      },
      "source": [
        "sequences"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  48,    9,   45, ...,  142,  468,   34],\n",
              "       [   9,   45,  220, ...,  468,   34,  278],\n",
              "       [  45,  220,  175, ...,   34,  278,  763],\n",
              "       ...,\n",
              "       [   7,  216,   17, ..., 1125, 1126, 1127],\n",
              "       [ 216,   17,  219, ..., 1126, 1127,   11],\n",
              "       [  17,  219,  162, ..., 1127,   11,   46]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMCthr0NVeJz",
        "colab_type": "text"
      },
      "source": [
        "# Creating an LSTM-based model\n",
        "\n",
        "Predict the last word in a sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFabKhuBVbKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,LSTM,Embedding # Embedding layer deals with vocabulary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaalrQWFVhL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PARAMETERS CHOICE\n",
        "\n",
        "# Activation = RELU\n",
        "# The size of the output layer is 'vocabulary_size'\n",
        "# Loss = 'categorical_crossentropy'\n",
        "\n",
        "def create_model(vocabulary_size, seq_len):\n",
        "    model = Sequential()\n",
        "    # Embedding turns positive integers(indexes) into dense vectors of fixed size (see docs).\n",
        "    model.add(Embedding(vocabulary_size, 20, input_length=seq_len)) \n",
        "    model.add(LSTM(150, return_sequences=True)) # better to take multiples of seq_len; smalle batches => faster\n",
        "    model.add(LSTM(150))\n",
        "    model.add(Dense(150, activation='relu'))\n",
        "\n",
        "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "   \n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdUDrScnVnzB",
        "colab_type": "text"
      },
      "source": [
        "## Feature / Label Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_lowGlOVjwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX_MXYseVrZe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "0a9091e5-4302-4e58-f547-0676f914f70f"
      },
      "source": [
        "# First 20 words (compare to 'sequences' : it's everything without the last index)\n",
        "sequences[:,:-1]"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  48,    9,   45, ...,   11,  142,  468],\n",
              "       [   9,   45,  220, ...,  142,  468,   34],\n",
              "       [  45,  220,  175, ...,  468,   34,  278],\n",
              "       ...,\n",
              "       [   7,  216,   17, ...,  174, 1125, 1126],\n",
              "       [ 216,   17,  219, ..., 1125, 1126, 1127],\n",
              "       [  17,  219,  162, ..., 1126, 1127,   11]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5zUGxMdVuBz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a6b71b0b-f98d-441c-8f68-c4e90d554c45"
      },
      "source": [
        "# last word\n",
        "sequences[:,-1]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  34,  278,  763, ..., 1127,   11,   46])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnGk4S6_V09V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e25b56c-c65f-48e4-86d6-6b58c0d94023"
      },
      "source": [
        "# X is the arrays of 20 words (sequences)\n",
        "\n",
        "X = sequences[:,:-1]\n",
        "\n",
        "# y (the target) is the 21st element\n",
        "y = sequences[:,-1]\n",
        "\n",
        "# one-hot\n",
        "y = to_categorical(y, num_classes=vocabulary_size+1)\n",
        "\n",
        "seq_len = X.shape[1]\n",
        "\n",
        "seq_len"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Qo09pXrWIEc",
        "colab_type": "text"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRytYaTQV3FS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "989b26f7-d6c6-4611-8acf-1e302772a5c0"
      },
      "source": [
        "# define model\n",
        "model = create_model(vocabulary_size+1, seq_len) # +1 for Embeddings"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 20, 20)            40100     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 20, 150)           102600    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 150)               180600    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 150)               22650     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2005)              302755    \n",
            "=================================================================\n",
            "Total params: 648,705\n",
            "Trainable params: 648,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ksgn_DeWKH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pickle import dump,load"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKxBR9NBWOBH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "12f1f961-fbf1-41ed-b721-40dea8938bca"
      },
      "source": [
        "# fit model\n",
        "\n",
        "# CAREFUL ! IT RUNS FOR ABOUT 2 HOURS ON CPU ! CHANGE TO GPU !\n",
        "\n",
        "model.fit(X, y, batch_size=128, epochs=300,verbose=1). # epochs: at least > 200"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/300\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "14464/14464 [==============================] - 25s 2ms/step - loss: 6.4483 - acc: 0.0422\n",
            "Epoch 2/300\n",
            "14464/14464 [==============================] - 7s 490us/step - loss: 6.0936 - acc: 0.0454\n",
            "Epoch 3/300\n",
            "14464/14464 [==============================] - 7s 486us/step - loss: 6.0334 - acc: 0.0454\n",
            "Epoch 4/300\n",
            "14464/14464 [==============================] - 7s 482us/step - loss: 5.8630 - acc: 0.0482\n",
            "Epoch 5/300\n",
            "14464/14464 [==============================] - 7s 498us/step - loss: 5.6244 - acc: 0.0666\n",
            "Epoch 6/300\n",
            "14464/14464 [==============================] - 7s 492us/step - loss: 5.3912 - acc: 0.0843\n",
            "Epoch 7/300\n",
            "14464/14464 [==============================] - 7s 495us/step - loss: 5.1898 - acc: 0.0951\n",
            "Epoch 8/300\n",
            "14464/14464 [==============================] - 7s 500us/step - loss: 5.0291 - acc: 0.1115\n",
            "Epoch 9/300\n",
            "14464/14464 [==============================] - 7s 495us/step - loss: 5.0667 - acc: 0.1126\n",
            "Epoch 10/300\n",
            "14464/14464 [==============================] - 7s 486us/step - loss: 4.8139 - acc: 0.1278\n",
            "Epoch 11/300\n",
            "14464/14464 [==============================] - 7s 495us/step - loss: 4.7106 - acc: 0.1359\n",
            "Epoch 12/300\n",
            "14464/14464 [==============================] - 7s 499us/step - loss: 4.7519 - acc: 0.1321\n",
            "Epoch 13/300\n",
            "14464/14464 [==============================] - 7s 493us/step - loss: 4.7280 - acc: 0.1377\n",
            "Epoch 14/300\n",
            "14464/14464 [==============================] - 7s 502us/step - loss: 4.5922 - acc: 0.1446\n",
            "Epoch 15/300\n",
            "14464/14464 [==============================] - 7s 507us/step - loss: 4.4749 - acc: 0.1551\n",
            "Epoch 16/300\n",
            "14464/14464 [==============================] - 7s 503us/step - loss: 4.4220 - acc: 0.1585\n",
            "Epoch 17/300\n",
            "14464/14464 [==============================] - 7s 506us/step - loss: 4.3651 - acc: 0.1637\n",
            "Epoch 18/300\n",
            "14464/14464 [==============================] - 7s 517us/step - loss: 4.2279 - acc: 0.1762\n",
            "Epoch 19/300\n",
            "14464/14464 [==============================] - 8s 525us/step - loss: 4.0895 - acc: 0.1854\n",
            "Epoch 20/300\n",
            "14464/14464 [==============================] - 7s 501us/step - loss: 3.9844 - acc: 0.1943\n",
            "Epoch 21/300\n",
            "14464/14464 [==============================] - 7s 506us/step - loss: 3.8653 - acc: 0.2011\n",
            "Epoch 22/300\n",
            "14464/14464 [==============================] - 7s 512us/step - loss: 3.7727 - acc: 0.2078\n",
            "Epoch 23/300\n",
            "14464/14464 [==============================] - 8s 520us/step - loss: 3.6636 - acc: 0.2155\n",
            "Epoch 24/300\n",
            "14464/14464 [==============================] - 7s 499us/step - loss: 3.5731 - acc: 0.2268\n",
            "Epoch 25/300\n",
            "14464/14464 [==============================] - 7s 507us/step - loss: 3.5446 - acc: 0.2262\n",
            "Epoch 26/300\n",
            "14464/14464 [==============================] - 7s 504us/step - loss: 3.4182 - acc: 0.2396\n",
            "Epoch 27/300\n",
            "14464/14464 [==============================] - 7s 516us/step - loss: 3.3174 - acc: 0.2511\n",
            "Epoch 28/300\n",
            "14464/14464 [==============================] - 7s 498us/step - loss: 3.2251 - acc: 0.2615\n",
            "Epoch 29/300\n",
            "14464/14464 [==============================] - 7s 511us/step - loss: 3.1399 - acc: 0.2725\n",
            "Epoch 30/300\n",
            "14464/14464 [==============================] - 7s 509us/step - loss: 3.0550 - acc: 0.2795\n",
            "Epoch 31/300\n",
            "14464/14464 [==============================] - 7s 503us/step - loss: 2.9749 - acc: 0.2938\n",
            "Epoch 32/300\n",
            "14464/14464 [==============================] - 7s 504us/step - loss: 2.8978 - acc: 0.3065\n",
            "Epoch 33/300\n",
            "14464/14464 [==============================] - 7s 518us/step - loss: 2.8226 - acc: 0.3231\n",
            "Epoch 34/300\n",
            "14464/14464 [==============================] - 7s 508us/step - loss: 2.7371 - acc: 0.3349\n",
            "Epoch 35/300\n",
            "14464/14464 [==============================] - 8s 519us/step - loss: 2.6640 - acc: 0.3523\n",
            "Epoch 36/300\n",
            "14464/14464 [==============================] - 7s 504us/step - loss: 2.5956 - acc: 0.3609\n",
            "Epoch 37/300\n",
            "14464/14464 [==============================] - 7s 493us/step - loss: 2.5158 - acc: 0.3789\n",
            "Epoch 38/300\n",
            "14464/14464 [==============================] - 7s 509us/step - loss: 2.4468 - acc: 0.3928\n",
            "Epoch 39/300\n",
            "14464/14464 [==============================] - 7s 499us/step - loss: 2.3824 - acc: 0.4095\n",
            "Epoch 40/300\n",
            "14464/14464 [==============================] - 7s 515us/step - loss: 2.3097 - acc: 0.4242\n",
            "Epoch 41/300\n",
            "14464/14464 [==============================] - 7s 503us/step - loss: 2.2527 - acc: 0.4336\n",
            "Epoch 42/300\n",
            "14464/14464 [==============================] - 7s 507us/step - loss: 2.1874 - acc: 0.4511\n",
            "Epoch 43/300\n",
            "14464/14464 [==============================] - 7s 503us/step - loss: 2.1191 - acc: 0.4611\n",
            "Epoch 44/300\n",
            "14464/14464 [==============================] - 7s 502us/step - loss: 2.0638 - acc: 0.4767\n",
            "Epoch 45/300\n",
            "14464/14464 [==============================] - 7s 490us/step - loss: 2.0088 - acc: 0.4871\n",
            "Epoch 46/300\n",
            "14464/14464 [==============================] - 7s 496us/step - loss: 1.9491 - acc: 0.5028\n",
            "Epoch 47/300\n",
            "14464/14464 [==============================] - 7s 503us/step - loss: 1.8960 - acc: 0.5159\n",
            "Epoch 48/300\n",
            "14464/14464 [==============================] - 7s 497us/step - loss: 1.8492 - acc: 0.5280\n",
            "Epoch 49/300\n",
            "14464/14464 [==============================] - 7s 494us/step - loss: 1.7993 - acc: 0.5398\n",
            "Epoch 50/300\n",
            "14464/14464 [==============================] - 7s 496us/step - loss: 1.7453 - acc: 0.5516\n",
            "Epoch 51/300\n",
            "14464/14464 [==============================] - 7s 512us/step - loss: 1.7003 - acc: 0.5604\n",
            "Epoch 52/300\n",
            "14464/14464 [==============================] - 7s 507us/step - loss: 1.6398 - acc: 0.5769\n",
            "Epoch 53/300\n",
            "14464/14464 [==============================] - 7s 479us/step - loss: 1.6051 - acc: 0.5865\n",
            "Epoch 54/300\n",
            "14464/14464 [==============================] - 7s 482us/step - loss: 1.5532 - acc: 0.6011\n",
            "Epoch 55/300\n",
            "14464/14464 [==============================] - 7s 484us/step - loss: 1.5081 - acc: 0.6125\n",
            "Epoch 56/300\n",
            "14464/14464 [==============================] - 7s 492us/step - loss: 1.4644 - acc: 0.6229\n",
            "Epoch 57/300\n",
            "14464/14464 [==============================] - 7s 479us/step - loss: 1.4232 - acc: 0.6316\n",
            "Epoch 58/300\n",
            "14464/14464 [==============================] - 7s 483us/step - loss: 1.3832 - acc: 0.6427\n",
            "Epoch 59/300\n",
            "14464/14464 [==============================] - 7s 493us/step - loss: 1.3545 - acc: 0.6466\n",
            "Epoch 60/300\n",
            "14464/14464 [==============================] - 7s 494us/step - loss: 1.3145 - acc: 0.6576\n",
            "Epoch 61/300\n",
            "14464/14464 [==============================] - 7s 483us/step - loss: 1.2772 - acc: 0.6640\n",
            "Epoch 62/300\n",
            "14464/14464 [==============================] - 7s 487us/step - loss: 1.2340 - acc: 0.6766\n",
            "Epoch 63/300\n",
            "14464/14464 [==============================] - 7s 496us/step - loss: 1.2052 - acc: 0.6878\n",
            "Epoch 64/300\n",
            "14464/14464 [==============================] - 7s 484us/step - loss: 1.1629 - acc: 0.7004\n",
            "Epoch 65/300\n",
            "14464/14464 [==============================] - 7s 503us/step - loss: 1.1278 - acc: 0.7084\n",
            "Epoch 66/300\n",
            "14464/14464 [==============================] - 7s 493us/step - loss: 1.0939 - acc: 0.7185\n",
            "Epoch 67/300\n",
            "14464/14464 [==============================] - 7s 485us/step - loss: 1.0659 - acc: 0.7255\n",
            "Epoch 68/300\n",
            "14464/14464 [==============================] - 7s 489us/step - loss: 1.0385 - acc: 0.7322\n",
            "Epoch 69/300\n",
            "14464/14464 [==============================] - 7s 493us/step - loss: 1.0000 - acc: 0.7455\n",
            "Epoch 70/300\n",
            "14464/14464 [==============================] - 7s 482us/step - loss: 0.9695 - acc: 0.7499\n",
            "Epoch 71/300\n",
            "14464/14464 [==============================] - 7s 487us/step - loss: 0.9338 - acc: 0.7638\n",
            "Epoch 72/300\n",
            "14464/14464 [==============================] - 7s 485us/step - loss: 0.9112 - acc: 0.7696\n",
            "Epoch 73/300\n",
            "14464/14464 [==============================] - 7s 497us/step - loss: 0.8963 - acc: 0.7698\n",
            "Epoch 74/300\n",
            "14464/14464 [==============================] - 7s 493us/step - loss: 0.8538 - acc: 0.7826\n",
            "Epoch 75/300\n",
            "14464/14464 [==============================] - 7s 489us/step - loss: 0.8315 - acc: 0.7933\n",
            "Epoch 76/300\n",
            "14464/14464 [==============================] - 7s 486us/step - loss: 0.7957 - acc: 0.7996\n",
            "Epoch 77/300\n",
            "14464/14464 [==============================] - 7s 490us/step - loss: 0.7781 - acc: 0.8058\n",
            "Epoch 78/300\n",
            "14464/14464 [==============================] - 7s 487us/step - loss: 0.7442 - acc: 0.8184\n",
            "Epoch 79/300\n",
            "14464/14464 [==============================] - 7s 484us/step - loss: 0.7242 - acc: 0.8247\n",
            "Epoch 80/300\n",
            "14464/14464 [==============================] - 7s 489us/step - loss: 0.7082 - acc: 0.8236\n",
            "Epoch 81/300\n",
            "14464/14464 [==============================] - 7s 487us/step - loss: 0.6740 - acc: 0.8355\n",
            "Epoch 82/300\n",
            "14464/14464 [==============================] - 7s 492us/step - loss: 0.6506 - acc: 0.8410\n",
            "Epoch 83/300\n",
            "14464/14464 [==============================] - 7s 490us/step - loss: 0.6197 - acc: 0.8480\n",
            "Epoch 84/300\n",
            "14464/14464 [==============================] - 7s 491us/step - loss: 0.6030 - acc: 0.8563\n",
            "Epoch 85/300\n",
            "14464/14464 [==============================] - 7s 494us/step - loss: 0.5892 - acc: 0.8605\n",
            "Epoch 86/300\n",
            "14464/14464 [==============================] - 7s 502us/step - loss: 0.5964 - acc: 0.8576\n",
            "Epoch 87/300\n",
            "14464/14464 [==============================] - 7s 484us/step - loss: 0.5580 - acc: 0.8671\n",
            "Epoch 88/300\n",
            "14464/14464 [==============================] - 7s 484us/step - loss: 0.5197 - acc: 0.8789\n",
            "Epoch 89/300\n",
            "14464/14464 [==============================] - 7s 485us/step - loss: 0.4929 - acc: 0.8846\n",
            "Epoch 90/300\n",
            "14464/14464 [==============================] - 7s 488us/step - loss: 0.4761 - acc: 0.8900\n",
            "Epoch 91/300\n",
            "14464/14464 [==============================] - 7s 485us/step - loss: 0.4519 - acc: 0.9011\n",
            "Epoch 92/300\n",
            "14464/14464 [==============================] - 7s 486us/step - loss: 0.4428 - acc: 0.9010\n",
            "Epoch 93/300\n",
            "14464/14464 [==============================] - 7s 489us/step - loss: 0.4279 - acc: 0.9054\n",
            "Epoch 94/300\n",
            "14464/14464 [==============================] - 7s 487us/step - loss: 0.4153 - acc: 0.9029\n",
            "Epoch 95/300\n",
            "14464/14464 [==============================] - 7s 488us/step - loss: 0.4057 - acc: 0.9109\n",
            "Epoch 96/300\n",
            "14464/14464 [==============================] - 7s 485us/step - loss: 0.3901 - acc: 0.9116\n",
            "Epoch 97/300\n",
            "14464/14464 [==============================] - 7s 498us/step - loss: 0.3664 - acc: 0.9200\n",
            "Epoch 98/300\n",
            "14464/14464 [==============================] - 7s 481us/step - loss: 0.3479 - acc: 0.9263\n",
            "Epoch 99/300\n",
            "14464/14464 [==============================] - 7s 492us/step - loss: 0.3211 - acc: 0.9348\n",
            "Epoch 100/300\n",
            "14464/14464 [==============================] - 7s 484us/step - loss: 0.3189 - acc: 0.9333\n",
            "Epoch 101/300\n",
            "14464/14464 [==============================] - 7s 492us/step - loss: 0.3087 - acc: 0.9369\n",
            "Epoch 102/300\n",
            "14464/14464 [==============================] - 7s 489us/step - loss: 0.3058 - acc: 0.9375\n",
            "Epoch 103/300\n",
            "14464/14464 [==============================] - 7s 496us/step - loss: 0.2854 - acc: 0.9397\n",
            "Epoch 104/300\n",
            "14464/14464 [==============================] - 7s 487us/step - loss: 0.2756 - acc: 0.9455\n",
            "Epoch 105/300\n",
            "14464/14464 [==============================] - 7s 485us/step - loss: 0.2564 - acc: 0.9502\n",
            "Epoch 106/300\n",
            "14464/14464 [==============================] - 7s 483us/step - loss: 0.2951 - acc: 0.9357\n",
            "Epoch 107/300\n",
            "14464/14464 [==============================] - 7s 492us/step - loss: 0.3082 - acc: 0.9307\n",
            "Epoch 108/300\n",
            "14464/14464 [==============================] - 7s 501us/step - loss: 0.2453 - acc: 0.9493\n",
            "Epoch 109/300\n",
            "14464/14464 [==============================] - 7s 504us/step - loss: 0.2005 - acc: 0.9654\n",
            "Epoch 110/300\n",
            "14464/14464 [==============================] - 7s 504us/step - loss: 0.1948 - acc: 0.9663\n",
            "Epoch 111/300\n",
            "14464/14464 [==============================] - 7s 493us/step - loss: 0.1757 - acc: 0.9712\n",
            "Epoch 112/300\n",
            "14464/14464 [==============================] - 7s 481us/step - loss: 0.1652 - acc: 0.9736\n",
            "Epoch 113/300\n",
            "14464/14464 [==============================] - 7s 485us/step - loss: 0.1514 - acc: 0.9788\n",
            "Epoch 114/300\n",
            "14464/14464 [==============================] - 7s 492us/step - loss: 0.1450 - acc: 0.9782\n",
            "Epoch 115/300\n",
            "14464/14464 [==============================] - 7s 482us/step - loss: 0.1418 - acc: 0.9811\n",
            "Epoch 116/300\n",
            "14464/14464 [==============================] - 7s 489us/step - loss: 0.1368 - acc: 0.9823\n",
            "Epoch 117/300\n",
            "14464/14464 [==============================] - 7s 492us/step - loss: 0.1293 - acc: 0.9829\n",
            "Epoch 118/300\n",
            "14464/14464 [==============================] - 7s 497us/step - loss: 0.1271 - acc: 0.9817\n",
            "Epoch 119/300\n",
            "14464/14464 [==============================] - 7s 491us/step - loss: 0.1507 - acc: 0.9764\n",
            "Epoch 120/300\n",
            "14464/14464 [==============================] - 7s 501us/step - loss: 0.1833 - acc: 0.9644\n",
            "Epoch 121/300\n",
            "14464/14464 [==============================] - 7s 483us/step - loss: 0.2055 - acc: 0.9551\n",
            "Epoch 122/300\n",
            "14464/14464 [==============================] - 7s 481us/step - loss: 0.2063 - acc: 0.9528\n",
            "Epoch 123/300\n",
            "14464/14464 [==============================] - 7s 486us/step - loss: 0.1825 - acc: 0.9620\n",
            "Epoch 124/300\n",
            "14464/14464 [==============================] - 7s 487us/step - loss: 0.1262 - acc: 0.9793\n",
            "Epoch 125/300\n",
            "14464/14464 [==============================] - 7s 498us/step - loss: 0.1035 - acc: 0.9867\n",
            "Epoch 126/300\n",
            "14464/14464 [==============================] - 7s 489us/step - loss: 0.0810 - acc: 0.9906\n",
            "Epoch 127/300\n",
            "14464/14464 [==============================] - 7s 489us/step - loss: 0.0677 - acc: 0.9934\n",
            "Epoch 128/300\n",
            "14464/14464 [==============================] - 7s 501us/step - loss: 0.0603 - acc: 0.9940\n",
            "Epoch 129/300\n",
            "14464/14464 [==============================] - 7s 483us/step - loss: 0.0565 - acc: 0.9947\n",
            "Epoch 130/300\n",
            "14464/14464 [==============================] - 7s 487us/step - loss: 0.0550 - acc: 0.9948\n",
            "Epoch 131/300\n",
            "14464/14464 [==============================] - 7s 490us/step - loss: 0.0496 - acc: 0.9951\n",
            "Epoch 132/300\n",
            "14464/14464 [==============================] - 7s 485us/step - loss: 0.0490 - acc: 0.9951\n",
            "Epoch 133/300\n",
            "14464/14464 [==============================] - 7s 489us/step - loss: 0.0491 - acc: 0.9953\n",
            "Epoch 134/300\n",
            "14464/14464 [==============================] - 7s 493us/step - loss: 0.0467 - acc: 0.9948\n",
            "Epoch 135/300\n",
            "14464/14464 [==============================] - 7s 486us/step - loss: 0.0495 - acc: 0.9949\n",
            "Epoch 136/300\n",
            "14464/14464 [==============================] - 7s 492us/step - loss: 0.0551 - acc: 0.9938\n",
            "Epoch 137/300\n",
            "14464/14464 [==============================] - 7s 486us/step - loss: 0.0745 - acc: 0.9888\n",
            "Epoch 138/300\n",
            "14464/14464 [==============================] - 7s 481us/step - loss: 0.5707 - acc: 0.8320\n",
            "Epoch 139/300\n",
            "14464/14464 [==============================] - 7s 481us/step - loss: 0.4594 - acc: 0.8590\n",
            "Epoch 140/300\n",
            "14464/14464 [==============================] - 7s 484us/step - loss: 0.1719 - acc: 0.9576\n",
            "Epoch 141/300\n",
            "14464/14464 [==============================] - 7s 495us/step - loss: 0.0795 - acc: 0.9882\n",
            "Epoch 142/300\n",
            "14464/14464 [==============================] - 7s 490us/step - loss: 0.0484 - acc: 0.9947\n",
            "Epoch 143/300\n",
            "14464/14464 [==============================] - 7s 478us/step - loss: 0.0392 - acc: 0.9955\n",
            "Epoch 144/300\n",
            "14464/14464 [==============================] - 7s 490us/step - loss: 0.0349 - acc: 0.9956\n",
            "Epoch 145/300\n",
            "14464/14464 [==============================] - 7s 500us/step - loss: 0.0327 - acc: 0.9958\n",
            "Epoch 146/300\n",
            "14464/14464 [==============================] - 7s 480us/step - loss: 0.0311 - acc: 0.9958\n",
            "Epoch 147/300\n",
            "14464/14464 [==============================] - 7s 480us/step - loss: 0.0312 - acc: 0.9958\n",
            "Epoch 148/300\n",
            "14464/14464 [==============================] - 7s 480us/step - loss: 0.0304 - acc: 0.9958\n",
            "Epoch 149/300\n",
            "14464/14464 [==============================] - 7s 482us/step - loss: 0.0286 - acc: 0.9961\n",
            "Epoch 150/300\n",
            "14464/14464 [==============================] - 7s 493us/step - loss: 0.0289 - acc: 0.9961\n",
            "Epoch 151/300\n",
            "14464/14464 [==============================] - 7s 488us/step - loss: 0.0283 - acc: 0.9958\n",
            "Epoch 152/300\n",
            "14464/14464 [==============================] - 7s 495us/step - loss: 0.0290 - acc: 0.9956\n",
            "Epoch 153/300\n",
            "14464/14464 [==============================] - 7s 504us/step - loss: 0.0273 - acc: 0.9959\n",
            "Epoch 154/300\n",
            "14464/14464 [==============================] - 7s 489us/step - loss: 0.0284 - acc: 0.9955\n",
            "Epoch 155/300\n",
            "14464/14464 [==============================] - 7s 478us/step - loss: 0.0275 - acc: 0.9961\n",
            "Epoch 156/300\n",
            "14464/14464 [==============================] - 7s 481us/step - loss: 0.0269 - acc: 0.9958\n",
            "Epoch 157/300\n",
            "14464/14464 [==============================] - 7s 484us/step - loss: 0.0306 - acc: 0.9956\n",
            "Epoch 158/300\n",
            "14464/14464 [==============================] - 7s 486us/step - loss: 0.0395 - acc: 0.9941\n",
            "Epoch 159/300\n",
            "14464/14464 [==============================] - 7s 483us/step - loss: 0.3320 - acc: 0.9011\n",
            "Epoch 160/300\n",
            "14464/14464 [==============================] - 7s 479us/step - loss: 0.5069 - acc: 0.8474\n",
            "Epoch 161/300\n",
            "14464/14464 [==============================] - 7s 486us/step - loss: 0.2121 - acc: 0.9407\n",
            "Epoch 162/300\n",
            "14464/14464 [==============================] - 7s 496us/step - loss: 0.0859 - acc: 0.9830\n",
            "Epoch 163/300\n",
            "14464/14464 [==============================] - 7s 484us/step - loss: 0.0397 - acc: 0.9943\n",
            "Epoch 164/300\n",
            "14464/14464 [==============================] - 7s 477us/step - loss: 0.0305 - acc: 0.9958\n",
            "Epoch 165/300\n",
            "14464/14464 [==============================] - 7s 488us/step - loss: 0.0255 - acc: 0.9956\n",
            "Epoch 166/300\n",
            "14464/14464 [==============================] - 7s 480us/step - loss: 0.0249 - acc: 0.9959\n",
            "Epoch 167/300\n",
            "14464/14464 [==============================] - 7s 488us/step - loss: 0.0229 - acc: 0.9959\n",
            "Epoch 168/300\n",
            "14464/14464 [==============================] - 7s 475us/step - loss: 0.0224 - acc: 0.9962\n",
            "Epoch 169/300\n",
            "14464/14464 [==============================] - 7s 480us/step - loss: 0.0225 - acc: 0.9961\n",
            "Epoch 170/300\n",
            "14464/14464 [==============================] - 7s 485us/step - loss: 0.0232 - acc: 0.9956\n",
            "Epoch 171/300\n",
            "14464/14464 [==============================] - 7s 491us/step - loss: 0.0218 - acc: 0.9961\n",
            "Epoch 172/300\n",
            "14464/14464 [==============================] - 7s 487us/step - loss: 0.0230 - acc: 0.9959\n",
            "Epoch 173/300\n",
            "14464/14464 [==============================] - 7s 478us/step - loss: 0.0232 - acc: 0.9958\n",
            "Epoch 174/300\n",
            "14464/14464 [==============================] - 7s 488us/step - loss: 0.0220 - acc: 0.9961\n",
            "Epoch 175/300\n",
            "14464/14464 [==============================] - 7s 487us/step - loss: 0.0216 - acc: 0.9961\n",
            "Epoch 176/300\n",
            "14464/14464 [==============================] - 7s 480us/step - loss: 0.0216 - acc: 0.9963\n",
            "Epoch 177/300\n",
            "14464/14464 [==============================] - 7s 487us/step - loss: 0.0236 - acc: 0.9958\n",
            "Epoch 178/300\n",
            "14464/14464 [==============================] - 7s 494us/step - loss: 0.0215 - acc: 0.9961\n",
            "Epoch 179/300\n",
            "14464/14464 [==============================] - 7s 494us/step - loss: 0.0222 - acc: 0.9960\n",
            "Epoch 180/300\n",
            "14464/14464 [==============================] - 7s 491us/step - loss: 0.0225 - acc: 0.9962\n",
            "Epoch 181/300\n",
            "14464/14464 [==============================] - 7s 479us/step - loss: 0.0224 - acc: 0.9961\n",
            "Epoch 182/300\n",
            "14464/14464 [==============================] - 7s 483us/step - loss: 0.0225 - acc: 0.9959\n",
            "Epoch 183/300\n",
            "14464/14464 [==============================] - 7s 485us/step - loss: 0.0239 - acc: 0.9956\n",
            "Epoch 184/300\n",
            "14464/14464 [==============================] - 7s 485us/step - loss: 0.0254 - acc: 0.9956\n",
            "Epoch 185/300\n",
            "14464/14464 [==============================] - 7s 501us/step - loss: 0.0551 - acc: 0.9878\n",
            "Epoch 186/300\n",
            "14464/14464 [==============================] - 7s 492us/step - loss: 1.1747 - acc: 0.7131\n",
            "Epoch 187/300\n",
            "14464/14464 [==============================] - 7s 494us/step - loss: 0.3001 - acc: 0.9094\n",
            "Epoch 188/300\n",
            "14464/14464 [==============================] - 7s 491us/step - loss: 0.0775 - acc: 0.9821\n",
            "Epoch 189/300\n",
            "14464/14464 [==============================] - 7s 488us/step - loss: 0.0343 - acc: 0.9945\n",
            "Epoch 190/300\n",
            "14464/14464 [==============================] - 7s 486us/step - loss: 0.0246 - acc: 0.9959\n",
            "Epoch 191/300\n",
            "14464/14464 [==============================] - 7s 482us/step - loss: 0.0217 - acc: 0.9959\n",
            "Epoch 192/300\n",
            "14464/14464 [==============================] - 7s 487us/step - loss: 0.0208 - acc: 0.9959\n",
            "Epoch 193/300\n",
            "14464/14464 [==============================] - 7s 491us/step - loss: 0.0206 - acc: 0.9959\n",
            "Epoch 194/300\n",
            "14464/14464 [==============================] - 7s 483us/step - loss: 0.0199 - acc: 0.9959\n",
            "Epoch 195/300\n",
            "14464/14464 [==============================] - 7s 478us/step - loss: 0.0184 - acc: 0.9964\n",
            "Epoch 196/300\n",
            "14464/14464 [==============================] - 7s 494us/step - loss: 0.0198 - acc: 0.9959\n",
            "Epoch 197/300\n",
            "14464/14464 [==============================] - 7s 502us/step - loss: 0.0207 - acc: 0.9957\n",
            "Epoch 198/300\n",
            "14464/14464 [==============================] - 7s 486us/step - loss: 0.0197 - acc: 0.9957\n",
            "Epoch 199/300\n",
            "14464/14464 [==============================] - 7s 493us/step - loss: 0.0187 - acc: 0.9959\n",
            "Epoch 200/300\n",
            "14464/14464 [==============================] - 7s 490us/step - loss: 0.0181 - acc: 0.9965\n",
            "Epoch 201/300\n",
            "14464/14464 [==============================] - 7s 493us/step - loss: 0.0199 - acc: 0.9958\n",
            "Epoch 202/300\n",
            "14464/14464 [==============================] - 7s 488us/step - loss: 0.0185 - acc: 0.9957\n",
            "Epoch 203/300\n",
            "14464/14464 [==============================] - 7s 486us/step - loss: 0.0188 - acc: 0.9959\n",
            "Epoch 204/300\n",
            "14464/14464 [==============================] - 7s 485us/step - loss: 0.0193 - acc: 0.9960\n",
            "Epoch 205/300\n",
            "14464/14464 [==============================] - 7s 488us/step - loss: 0.0186 - acc: 0.9961\n",
            "Epoch 206/300\n",
            "14464/14464 [==============================] - 7s 482us/step - loss: 0.0182 - acc: 0.9963\n",
            "Epoch 207/300\n",
            "14464/14464 [==============================] - 7s 479us/step - loss: 0.0207 - acc: 0.9960\n",
            "Epoch 208/300\n",
            "14464/14464 [==============================] - 7s 482us/step - loss: 0.0191 - acc: 0.9963\n",
            "Epoch 209/300\n",
            "14464/14464 [==============================] - 7s 485us/step - loss: 0.0200 - acc: 0.9959\n",
            "Epoch 210/300\n",
            "14464/14464 [==============================] - 7s 496us/step - loss: 0.0205 - acc: 0.9958\n",
            "Epoch 211/300\n",
            "14464/14464 [==============================] - 7s 486us/step - loss: 0.0739 - acc: 0.9803\n",
            "Epoch 212/300\n",
            "14464/14464 [==============================] - 7s 490us/step - loss: 0.6395 - acc: 0.8124\n",
            "Epoch 213/300\n",
            "14464/14464 [==============================] - 7s 490us/step - loss: 0.2911 - acc: 0.9114\n",
            "Epoch 214/300\n",
            "14464/14464 [==============================] - 7s 488us/step - loss: 0.0877 - acc: 0.9774\n",
            "Epoch 215/300\n",
            "14464/14464 [==============================] - 7s 499us/step - loss: 0.0490 - acc: 0.9907\n",
            "Epoch 216/300\n",
            "14464/14464 [==============================] - 7s 497us/step - loss: 0.0412 - acc: 0.9919\n",
            "Epoch 217/300\n",
            "14464/14464 [==============================] - 7s 480us/step - loss: 0.0306 - acc: 0.9939\n",
            "Epoch 218/300\n",
            "14464/14464 [==============================] - 7s 497us/step - loss: 0.0233 - acc: 0.9950\n",
            "Epoch 219/300\n",
            "14464/14464 [==============================] - 7s 490us/step - loss: 0.0193 - acc: 0.9956\n",
            "Epoch 220/300\n",
            "14464/14464 [==============================] - 7s 491us/step - loss: 0.0185 - acc: 0.9956\n",
            "Epoch 221/300\n",
            "14464/14464 [==============================] - 7s 497us/step - loss: 0.0164 - acc: 0.9965\n",
            "Epoch 222/300\n",
            "14464/14464 [==============================] - 7s 504us/step - loss: 0.0165 - acc: 0.9963\n",
            "Epoch 223/300\n",
            "14464/14464 [==============================] - 7s 486us/step - loss: 0.0166 - acc: 0.9961\n",
            "Epoch 224/300\n",
            "14464/14464 [==============================] - 7s 491us/step - loss: 0.0168 - acc: 0.9962\n",
            "Epoch 225/300\n",
            "14464/14464 [==============================] - 7s 492us/step - loss: 0.0169 - acc: 0.9962\n",
            "Epoch 226/300\n",
            "14464/14464 [==============================] - 7s 492us/step - loss: 0.0171 - acc: 0.9959\n",
            "Epoch 227/300\n",
            "14464/14464 [==============================] - 7s 492us/step - loss: 0.0176 - acc: 0.9958\n",
            "Epoch 228/300\n",
            "14464/14464 [==============================] - 7s 490us/step - loss: 0.0161 - acc: 0.9963\n",
            "Epoch 229/300\n",
            "14464/14464 [==============================] - 7s 495us/step - loss: 0.0166 - acc: 0.9963\n",
            "Epoch 230/300\n",
            "14464/14464 [==============================] - 7s 497us/step - loss: 0.0173 - acc: 0.9962\n",
            "Epoch 231/300\n",
            "14464/14464 [==============================] - 7s 478us/step - loss: 0.0168 - acc: 0.9961\n",
            "Epoch 232/300\n",
            "14464/14464 [==============================] - 7s 487us/step - loss: 0.0181 - acc: 0.9959\n",
            "Epoch 233/300\n",
            "14464/14464 [==============================] - 7s 497us/step - loss: 0.0171 - acc: 0.9960\n",
            "Epoch 234/300\n",
            "14464/14464 [==============================] - 7s 489us/step - loss: 0.0187 - acc: 0.9956\n",
            "Epoch 235/300\n",
            "14464/14464 [==============================] - 7s 488us/step - loss: 0.0181 - acc: 0.9962\n",
            "Epoch 236/300\n",
            "14464/14464 [==============================] - 7s 491us/step - loss: 0.0176 - acc: 0.9960\n",
            "Epoch 237/300\n",
            "14464/14464 [==============================] - 7s 493us/step - loss: 0.0205 - acc: 0.9958\n",
            "Epoch 238/300\n",
            "14464/14464 [==============================] - 7s 491us/step - loss: 0.0235 - acc: 0.9954\n",
            "Epoch 239/300\n",
            "14464/14464 [==============================] - 7s 505us/step - loss: 0.0316 - acc: 0.9932\n",
            "Epoch 240/300\n",
            "14464/14464 [==============================] - 7s 496us/step - loss: 0.5540 - acc: 0.8388\n",
            "Epoch 241/300\n",
            "14464/14464 [==============================] - 7s 503us/step - loss: 0.3701 - acc: 0.8850\n",
            "Epoch 242/300\n",
            "14464/14464 [==============================] - 7s 496us/step - loss: 0.0948 - acc: 0.9721\n",
            "Epoch 243/300\n",
            "14464/14464 [==============================] - 7s 489us/step - loss: 0.0314 - acc: 0.9943\n",
            "Epoch 244/300\n",
            "14464/14464 [==============================] - 7s 485us/step - loss: 0.0192 - acc: 0.9957\n",
            "Epoch 245/300\n",
            "14464/14464 [==============================] - 7s 489us/step - loss: 0.0172 - acc: 0.9959\n",
            "Epoch 246/300\n",
            "14464/14464 [==============================] - 7s 494us/step - loss: 0.0157 - acc: 0.9961\n",
            "Epoch 247/300\n",
            "14464/14464 [==============================] - 7s 489us/step - loss: 0.0160 - acc: 0.9961\n",
            "Epoch 248/300\n",
            "14464/14464 [==============================] - 7s 489us/step - loss: 0.0153 - acc: 0.9963\n",
            "Epoch 249/300\n",
            "14464/14464 [==============================] - 7s 484us/step - loss: 0.0164 - acc: 0.9956\n",
            "Epoch 250/300\n",
            "14464/14464 [==============================] - 7s 487us/step - loss: 0.0153 - acc: 0.9963\n",
            "Epoch 251/300\n",
            "14464/14464 [==============================] - 7s 487us/step - loss: 0.0150 - acc: 0.9964\n",
            "Epoch 252/300\n",
            "14464/14464 [==============================] - 7s 494us/step - loss: 0.0150 - acc: 0.9963\n",
            "Epoch 253/300\n",
            "14464/14464 [==============================] - 7s 492us/step - loss: 0.0159 - acc: 0.9956\n",
            "Epoch 254/300\n",
            "14464/14464 [==============================] - 7s 497us/step - loss: 0.0151 - acc: 0.9963\n",
            "Epoch 255/300\n",
            "14464/14464 [==============================] - 7s 493us/step - loss: 0.0163 - acc: 0.9961\n",
            "Epoch 256/300\n",
            "14464/14464 [==============================] - 7s 502us/step - loss: 0.0150 - acc: 0.9963\n",
            "Epoch 257/300\n",
            "14464/14464 [==============================] - 7s 505us/step - loss: 0.0160 - acc: 0.9963\n",
            "Epoch 258/300\n",
            "14464/14464 [==============================] - 7s 491us/step - loss: 0.0149 - acc: 0.9963\n",
            "Epoch 259/300\n",
            "14464/14464 [==============================] - 7s 480us/step - loss: 0.0170 - acc: 0.9957\n",
            "Epoch 260/300\n",
            "14464/14464 [==============================] - 7s 499us/step - loss: 0.0167 - acc: 0.9955\n",
            "Epoch 261/300\n",
            "14464/14464 [==============================] - 7s 490us/step - loss: 0.0163 - acc: 0.9963\n",
            "Epoch 262/300\n",
            "14464/14464 [==============================] - 7s 489us/step - loss: 0.0178 - acc: 0.9961\n",
            "Epoch 263/300\n",
            "14464/14464 [==============================] - 7s 497us/step - loss: 0.0186 - acc: 0.9961\n",
            "Epoch 264/300\n",
            "14464/14464 [==============================] - 7s 496us/step - loss: 0.0295 - acc: 0.9928\n",
            "Epoch 265/300\n",
            "14464/14464 [==============================] - 7s 482us/step - loss: 0.5307 - acc: 0.8462\n",
            "Epoch 266/300\n",
            "14464/14464 [==============================] - 7s 480us/step - loss: 0.2716 - acc: 0.9162\n",
            "Epoch 267/300\n",
            "14464/14464 [==============================] - 7s 497us/step - loss: 0.0862 - acc: 0.9751\n",
            "Epoch 268/300\n",
            "14464/14464 [==============================] - 7s 501us/step - loss: 0.0294 - acc: 0.9939\n",
            "Epoch 269/300\n",
            "14464/14464 [==============================] - 7s 493us/step - loss: 0.0182 - acc: 0.9959\n",
            "Epoch 270/300\n",
            "14464/14464 [==============================] - 7s 495us/step - loss: 0.0149 - acc: 0.9964\n",
            "Epoch 271/300\n",
            "14464/14464 [==============================] - 7s 500us/step - loss: 0.0153 - acc: 0.9962\n",
            "Epoch 272/300\n",
            "14464/14464 [==============================] - 7s 495us/step - loss: 0.0147 - acc: 0.9961\n",
            "Epoch 273/300\n",
            "14464/14464 [==============================] - 7s 502us/step - loss: 0.0140 - acc: 0.9965\n",
            "Epoch 274/300\n",
            "14464/14464 [==============================] - 7s 493us/step - loss: 0.0143 - acc: 0.9961\n",
            "Epoch 275/300\n",
            "14464/14464 [==============================] - 7s 489us/step - loss: 0.0153 - acc: 0.9959\n",
            "Epoch 276/300\n",
            "14464/14464 [==============================] - 7s 487us/step - loss: 0.0146 - acc: 0.9961\n",
            "Epoch 277/300\n",
            "14464/14464 [==============================] - 7s 501us/step - loss: 0.0142 - acc: 0.9962\n",
            "Epoch 278/300\n",
            "14464/14464 [==============================] - 7s 496us/step - loss: 0.0143 - acc: 0.9965\n",
            "Epoch 279/300\n",
            "14464/14464 [==============================] - 7s 506us/step - loss: 0.0145 - acc: 0.9963\n",
            "Epoch 280/300\n",
            "14464/14464 [==============================] - 7s 501us/step - loss: 0.0156 - acc: 0.9958\n",
            "Epoch 281/300\n",
            "14464/14464 [==============================] - 7s 499us/step - loss: 0.0163 - acc: 0.9956\n",
            "Epoch 282/300\n",
            "14464/14464 [==============================] - 7s 490us/step - loss: 0.0148 - acc: 0.9965\n",
            "Epoch 283/300\n",
            "14464/14464 [==============================] - 7s 493us/step - loss: 0.0146 - acc: 0.9963\n",
            "Epoch 284/300\n",
            "14464/14464 [==============================] - 7s 512us/step - loss: 0.0158 - acc: 0.9957\n",
            "Epoch 285/300\n",
            "14464/14464 [==============================] - 7s 505us/step - loss: 0.0161 - acc: 0.9961\n",
            "Epoch 286/300\n",
            "14464/14464 [==============================] - 7s 496us/step - loss: 0.0168 - acc: 0.9956\n",
            "Epoch 287/300\n",
            "14464/14464 [==============================] - 7s 492us/step - loss: 0.0170 - acc: 0.9956\n",
            "Epoch 288/300\n",
            "14464/14464 [==============================] - 7s 495us/step - loss: 0.0163 - acc: 0.9960\n",
            "Epoch 289/300\n",
            "14464/14464 [==============================] - 7s 509us/step - loss: 0.0157 - acc: 0.9961\n",
            "Epoch 290/300\n",
            "14464/14464 [==============================] - 7s 499us/step - loss: 0.0172 - acc: 0.9957\n",
            "Epoch 291/300\n",
            "14464/14464 [==============================] - 7s 488us/step - loss: 0.0167 - acc: 0.9960\n",
            "Epoch 292/300\n",
            "14464/14464 [==============================] - 7s 497us/step - loss: 0.2894 - acc: 0.9197\n",
            "Epoch 293/300\n",
            "14464/14464 [==============================] - 7s 499us/step - loss: 0.4741 - acc: 0.8633\n",
            "Epoch 294/300\n",
            "14464/14464 [==============================] - 7s 492us/step - loss: 0.1040 - acc: 0.9699\n",
            "Epoch 295/300\n",
            "14464/14464 [==============================] - 7s 497us/step - loss: 0.0383 - acc: 0.9912\n",
            "Epoch 296/300\n",
            "14464/14464 [==============================] - 7s 500us/step - loss: 0.0203 - acc: 0.9954\n",
            "Epoch 297/300\n",
            "14464/14464 [==============================] - 7s 494us/step - loss: 0.0154 - acc: 0.9963\n",
            "Epoch 298/300\n",
            "14464/14464 [==============================] - 7s 500us/step - loss: 0.0151 - acc: 0.9959\n",
            "Epoch 299/300\n",
            "14464/14464 [==============================] - 7s 499us/step - loss: 0.0145 - acc: 0.9962\n",
            "Epoch 300/300\n",
            "14464/14464 [==============================] - 7s 505us/step - loss: 0.0139 - acc: 0.9960\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f98e3aa1080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_qqT-LMM0F3",
        "colab_type": "text"
      },
      "source": [
        "# Generating New Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPO2UHJmWQUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import randint\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCgB11oILsMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
        "    '''\n",
        "    INPUTS:\n",
        "    model : model that was trained on text data\n",
        "    tokenizer : tokenizer that was fit on text data\n",
        "    seq_len : length of training sequence\n",
        "    seed_text : raw string text to serve as the seed\n",
        "    num_gen_words : number of words to be generated by model\n",
        "    '''\n",
        "    \n",
        "    # Final Output\n",
        "    output_text = []\n",
        "    \n",
        "    # Intial Seed Sequence\n",
        "    input_text = seed_text\n",
        "    \n",
        "    # Create num_gen_words\n",
        "    for i in range(num_gen_words):\n",
        "        \n",
        "        # Take the input text string and encode it to a sequence\n",
        "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
        "        \n",
        "        # Pad sequences to our trained rate \n",
        "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
        "        \n",
        "        # Predict Class Probabilities for each word\n",
        "        pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0] # [0] returns index \n",
        "        \n",
        "        # Grab word\n",
        "        pred_word = tokenizer.index_word[pred_word_ind] \n",
        "        \n",
        "        # Update the sequence of input text (shifting one over with the new word)\n",
        "        input_text += ' ' + pred_word\n",
        "        \n",
        "        output_text.append(pred_word)\n",
        "        \n",
        "    # Make it look like a sentence.\n",
        "    return ' '.join(output_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuPLHRVOQKu_",
        "colab_type": "text"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jglac3p0QJTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('LSTM_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDvohqQ6M2nA",
        "colab_type": "text"
      },
      "source": [
        "## Grab a random seed sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKnFY1JHLym1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "7353f7b5-45dd-4a62-e68d-6faa583b906f"
      },
      "source": [
        "text_sequences[500]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['de',\n",
              " 'services',\n",
              " 'entrepreneur',\n",
              " '21',\n",
              " '\\n  ',\n",
              " '7.01',\n",
              " 'statut',\n",
              " 'importante',\n",
              " '22',\n",
              " '\\n  ',\n",
              " '7.02',\n",
              " 'capacité',\n",
              " 'importante',\n",
              " '22',\n",
              " '\\n  ',\n",
              " '7.03',\n",
              " 'divulgation',\n",
              " 'importante',\n",
              " '22',\n",
              " '\\n  ',\n",
              " '7.04']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GRfTfQENCk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "random.seed(101)\n",
        "random_pick = random.randint(0,len(text_sequences))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZB-cEtYNGkZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "c8ae5c42-0a80-410a-df39-6193e9639796"
      },
      "source": [
        "random_seed_text = text_sequences[random_pick]\n",
        "random_seed_text"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cadre',\n",
              " 'd’',\n",
              " 'un',\n",
              " 'contrat',\n",
              " 'antérieur',\n",
              " 'avec',\n",
              " 'un',\n",
              " 'organisme',\n",
              " 'public',\n",
              " 'du',\n",
              " 'québec',\n",
              " 'fait',\n",
              " 'l’',\n",
              " 'objet',\n",
              " 'd’',\n",
              " 'une',\n",
              " 'évaluation',\n",
              " 'de',\n",
              " 'rendement',\n",
              " 'insatisfaisant',\n",
              " 'de']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz-uwW-8NuAL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9842a460-7f65-41f4-b776-d0eb43c1ea1a"
      },
      "source": [
        "seed_text = ' '.join(random_seed_text)\n",
        "seed_text"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cadre d’ un contrat antérieur avec un organisme public du québec fait l’ objet d’ une évaluation de rendement insatisfaisant de'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByD07v2wNz-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07ea7ddd-97ae-4712-9815-6e5433268be1"
      },
      "source": [
        "## GENERATED NEW TEXT !!!\n",
        "\n",
        "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=20)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'la part de cet organisme public ne pas faire l’ objet d’ une requête en faillite volontaire ou involontaire ou'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUNh3PrIQio5",
        "colab_type": "text"
      },
      "source": [
        "## Exploring generated sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTnP70eEN6sY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "8b595ca6-9ba1-4388-9816-9333ced4bf73"
      },
      "source": [
        "for i,word in enumerate(text.split()):\n",
        "    if word == 'organisme':\n",
        "        print(' '.join(text.split()[i-20:i+20]))\n",
        "        print('\\n')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "redevable d’un montant exigible en vertu d’une loi fiscale ou alimentaire, l’ORGANISME PUBLIC, étant ou agissant pour le compte d’un organisme public tel que défini à l’article 31.1.4 de la Loi sur l’administration fiscale, peut, s’il en est requis par\n",
            "\n",
            "\n",
            "tel consentement doit notamment respecter les critères suivants : ne pas avoir, dans le cadre d’un contrat antérieur avec un organisme public du Québec, fait l’objet d’une évaluation de rendement insatisfaisant de la part de cet organisme public; ne pas\n",
            "\n",
            "\n",
            "contrat antérieur avec un organisme public du Québec, fait l’objet d’une évaluation de rendement insatisfaisant de la part de cet organisme public; ne pas faire l’objet d’une requête en faillite volontaire ou involontaire ou de toute autre procédure relative à\n",
            "\n",
            "\n",
            "dernier qui ne peut s’y opposer sans motif sérieux, ajouter, aux mêmes termes et conditions, d’autres établissements membres de son organisme parmi ceux indiqués à l’annexe A - Liste des Établissements Participants, dans la section «Établissements membres intéressés». SIGNATURE LES\n",
            "\n",
            "\n",
            "de la déclarante) ANNEXE 10.15 B - FICHE D'INFORMATION SUR LA DESTRUCTION DES DOCUMENTS CONTENANT DES RENSEIGNEMENTS PERSONNELS [Facultative] Tout organisme public ou toute entreprise privée qui recueillent, détiennent, utilisent ou communiquent des renseignements personnels doivent mettre en place des\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BHaRXhLQ3rq",
        "colab_type": "text"
      },
      "source": [
        "## To reuse the model, load it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LdDSORLOigs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('LSTM_model.h5')\n",
        "tokenizer =load(open('LSTM_model', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VPJaHY9OqD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRQr_m59Op8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzoJJ75TOp4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}